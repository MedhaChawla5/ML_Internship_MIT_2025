{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5ff782-d4e1-4470-83b1-ff0941ddd75a",
   "metadata": {},
   "source": [
    "# Penguin size - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976a0df1-8ee9-49c9-8ae0-c78aa698d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0752ec79-6e0e-4493-8ea8-4724630f718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"penguins.csv\")\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edbadb8-dc45-4edf-a086-4a651f02a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'] = df['species'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b25d99-0b18-49a1-a027-eaa9605ea420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "4      0\n",
       "5      0\n",
       "      ..\n",
       "339    1\n",
       "340    1\n",
       "341    1\n",
       "342    1\n",
       "343    1\n",
       "Name: species, Length: 333, dtype: int8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c763f763-38a6-4b44-af00-7cf5c7e1f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values\n",
    "labels = df['species'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754cf4bc-3a7b-4c11-986e-5373119d2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71eabed2-764e-4786-a436-9fd4b4ff335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31314bee-e2c2-49f8-b9c2-37f19d8a6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, x_test, k=3):\n",
    "    distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "    most_common = Counter(k_nearest_labels).most_common(1)\n",
    "    return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315cb3bb-be2c-471e-a4b2-05199f02a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "y_pred = [knn_predict(X_train, y_train, x, k) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9255a14e-66a4-46c8-b4c6-7ebfe4a6f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = np.unique(y_true)\n",
    "    metrics = {}\n",
    "    for label in labels:\n",
    "        TP = sum((y_pred[i] == label) and (y_true[i] == label) for i in range(len(y_true)))\n",
    "        FP = sum((y_pred[i] == label) and (y_true[i] != label) for i in range(len(y_true)))\n",
    "        FN = sum((y_pred[i] != label) and (y_true[i] == label) for i in range(len(y_true)))\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        metrics[label] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "    accuracy = np.mean(np.array(y_pred) == np.array(y_true))\n",
    "    return accuracy, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a5b177-6964-4899-ab5c-7c058d626a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731\n",
      "Class-wise Metrics:\n",
      "Class 0 -> Precision: 0.68, Recall: 0.81, F1: 0.74\n",
      "Class 1 -> Precision: 0.88, Recall: 0.39, F1: 0.54\n",
      "Class 2 -> Precision: 0.77, Recall: 0.94, F1: 0.85\n"
     ]
    }
   ],
   "source": [
    "accuracy, class_metrics = evaluate(y_test, y_pred)\n",
    "print(\"Accuracy:\", round(accuracy, 3))\n",
    "print(\"Class-wise Metrics:\")\n",
    "for label, m in class_metrics.items():\n",
    "    print(f\"Class {label} -> Precision: {m['Precision']:.2f}, Recall: {m['Recall']:.2f}, F1: {m['F1-Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81db684-889d-43ff-b281-39a1feca48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 Predictions:\n",
      "Sample 1: Predicted = 1, Actual = 0\n",
      "Sample 2: Predicted = 1, Actual = 1\n",
      "Sample 3: Predicted = 0, Actual = 0\n",
      "Sample 4: Predicted = 2, Actual = 2\n",
      "Sample 5: Predicted = 0, Actual = 0\n",
      "Sample 6: Predicted = 0, Actual = 1\n",
      "Sample 7: Predicted = 0, Actual = 1\n",
      "Sample 8: Predicted = 2, Actual = 2\n",
      "Sample 9: Predicted = 2, Actual = 2\n",
      "Sample 10: Predicted = 2, Actual = 2\n"
     ]
    }
   ],
   "source": [
    "species_map = dict(enumerate(df['species'].astype('category').cat.categories))\n",
    "\n",
    "print(\"\\nFirst 10 Predictions:\")\n",
    "for i in range(10):\n",
    "    actual = species_map[y_test[i]]\n",
    "    predicted = species_map[y_pred[i]]\n",
    "    print(f\"Sample {i+1}: Predicted = {predicted}, Actual = {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94baa831-511e-4c92-9feb-6fac480bd1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy for different values of k:\n",
      "\n",
      "k = 1 --> Accuracy: 0.776\n",
      "k = 2 --> Accuracy: 0.776\n",
      "k = 3 --> Accuracy: 0.746\n",
      "k = 4 --> Accuracy: 0.731\n",
      "k = 5 --> Accuracy: 0.731\n",
      "k = 6 --> Accuracy: 0.746\n",
      "k = 7 --> Accuracy: 0.761\n",
      "k = 8 --> Accuracy: 0.716\n",
      "k = 9 --> Accuracy: 0.687\n",
      "k = 10 --> Accuracy: 0.701\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating accuracy for different values of k:\\n\")\n",
    "for k in range(1, 11):  # Try k from 1 to 10\n",
    "    y_pred_k = [knn_predict(X_train, y_train, x, k) for x in X_test]\n",
    "    accuracy_k, _ = evaluate(y_test, y_pred_k)\n",
    "    print(f\"k = {k} --> Accuracy: {accuracy_k:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84499405-7a61-469c-9992-4a31bb58a74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
