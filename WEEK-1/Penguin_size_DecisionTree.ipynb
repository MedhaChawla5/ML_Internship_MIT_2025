{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373f4085-b92e-4abc-bceb-9892e8743228",
   "metadata": {},
   "source": [
    "# Penguin-size-Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee1394a-0d1d-4c01-b9e4-0b4f07f0786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798e27a6-c190-4fe7-bc43-5862336cd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"penguins.csv\")\n",
    "df = df.dropna()\n",
    "df['species'] = df['species'].astype('category').cat.codes\n",
    "\n",
    "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "X = df[features].values\n",
    "y = df['species'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4903ed-18fe-417e-892c-61457418b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    counts = np.bincount(labels)\n",
    "    probabilities = counts / len(labels)\n",
    "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a5fba0-abd7-429b-a188-45677a860b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(parent_labels, left_labels, right_labels):\n",
    "    parent_entropy = entropy(parent_labels)\n",
    "    n = len(parent_labels)\n",
    "    n_left = len(left_labels)\n",
    "    n_right = len(right_labels)\n",
    "    weighted_entropy = (n_left / n) * entropy(left_labels) + (n_right / n) * entropy(right_labels)\n",
    "    return parent_entropy - weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5a817c-a713-47ab-ab4f-c4c69129529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_feature_for_point(X_train, y_train, x_test):\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_ig = -1\n",
    "    \n",
    "    num_features = X_train.shape[1]\n",
    "    \n",
    "    for feature_idx in range(num_features):\n",
    "        unique_values = np.unique(X_train[:, feature_idx])\n",
    "        \n",
    "        for thresh in unique_values:\n",
    "            left_mask = X_train[:, feature_idx] <= thresh\n",
    "            right_mask = X_train[:, feature_idx] > thresh\n",
    "            if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            ig = info_gain(y_train, y_train[left_mask], y_train[right_mask])\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_feature = feature_idx\n",
    "                best_threshold = thresh\n",
    "    \n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8ab050-0ea8-488f-b765-f2ec924d0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_point(X_train, y_train, x_test):\n",
    "    feature, thresh = best_split_feature_for_point(X_train, y_train, x_test)\n",
    "    if feature is None:\n",
    "        return Counter(y_train).most_common(1)[0][0]\n",
    "    if x_test[feature] <= thresh:\n",
    "        labels_left = y_train[X_train[:, feature] <= thresh]\n",
    "        return Counter(labels_left).most_common(1)[0][0]\n",
    "    else:\n",
    "        labels_right = y_train[X_train[:, feature] > thresh]\n",
    "        return Counter(labels_right).most_common(1)[0][0]\n",
    "\n",
    "y_pred = [predict_single_point(X_train, y_train, x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ebf025-580a-446a-8368-511e12604564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = np.unique(y_true)\n",
    "    metrics = {}\n",
    "    for label in labels:\n",
    "        TP = sum((np.array(y_pred) == label) & (y_true == label))\n",
    "        FP = sum((np.array(y_pred) == label) & (y_true != label))\n",
    "        FN = sum((np.array(y_pred) != label) & (y_true == label))\n",
    "        precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "        metrics[label] = {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "    accuracy = np.mean(np.array(y_pred) == y_true)\n",
    "    return accuracy, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f907ff5a-2447-4c26-944e-e6a19bb1cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731\n",
      "Class-wise Metrics:\n",
      "Class 0 -> Precision: 0.65, Recall: 1.00, F1: 0.78\n",
      "Class 1 -> Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Class 2 -> Precision: 0.95, Recall: 1.00, F1: 0.97\n"
     ]
    }
   ],
   "source": [
    "accuracy, class_metrics = evaluate(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Class-wise Metrics:\")\n",
    "for label, m in class_metrics.items():\n",
    "    print(f\"Class {label} -> Precision: {m['Precision']:.2f}, Recall: {m['Recall']:.2f}, F1: {m['F1-Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d4557a5-d147-4761-9797-7342b31a3aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example predictions (Actual vs Predicted):\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 2, Predicted: 2\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 0\n",
      "Actual: 1, Predicted: 0\n",
      "Actual: 2, Predicted: 2\n",
      "Actual: 2, Predicted: 2\n",
      "Actual: 2, Predicted: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample predictions (Actual vs Predicted):\")\n",
    "for i in range(10):\n",
    "    print(f\"Actual: {y_test[i]}, Predicted: {y_pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ab83c-f19f-4eb0-bd57-0f216aa37306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
